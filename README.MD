# Long text summarization using local LLM

Uses mistral to summarize a book. Chunks the long text (free ebook) and summarizes each chunk.
The chunks are then fed to the model for final summarization.
